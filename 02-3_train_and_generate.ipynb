{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from parse_preprocessed_data import get_inputs_and_targets\n",
    "from LSTM_Model import LSTMModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These hyper-paramters are identical to the Github's to test differences\n",
    "seq_length = 50\n",
    "\n",
    "hidden_size = 128\n",
    "learning_rate = 2e-3\n",
    "dropout = 0.5\n",
    "batch_size = 100\n",
    "num_layers = 3\n",
    "max_epochs = 20\n",
    "validation_prop = 0.2\n",
    "\n",
    "snaking = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique chars: ['\\n', '-', '<', '>', '?', 'B', 'E', 'Q', 'S', 'X', '[', ']', 'b', 'o', 'x']\n",
      "Number of unique chars: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(15, (121451, 50, 15), (121451, 50))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Not Original\n",
    "char_to_ix, ix_to_char, vocab_size, inputs, targets = get_inputs_and_targets('data_preprocessed/mario.txt', seq_length, snaking)\n",
    "vocab_size, inputs.shape, targets.shape\n",
    "\n",
    "#vocab_size is 15\n",
    "#inputs is (num_sequences, seq_length, vocab_size), and is one-hot encoded\n",
    "#targets is (num_sequence, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the starting seed\n",
    "first_three_cols = inputs[0][:3 * 17]\n",
    "np.savetxt('data_preprocessed/seed.txt', first_three_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the inputs and targets into tensors and load them into a dataset, then DataLoader\n",
    "inputs_tensor = torch.tensor(inputs)\n",
    "targets_tensor = torch.tensor(targets)\n",
    "\n",
    "train_dataset = TensorDataset(inputs_tensor, targets_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize model\n",
    "model = LSTMModel(vocab_size, hidden_size, num_layers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the loss and optimizer functions\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "\n",
    "#Lists to store loss and accuracy values for each epoch\n",
    "loss_values = []\n",
    "accuracy_values = []\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for batch_inputs, batch_targets in train_loader:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        #Clear gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Forward pass\n",
    "        outputs, _ = model(batch_inputs)\n",
    "        \n",
    "        #Reshape outputs and targets to have the same batch size\n",
    "        outputs_flat = outputs.view(-1, outputs.size(-1))\n",
    "        targets_flat = batch_targets.view(-1)\n",
    "        \n",
    "        #Calculate loss\n",
    "        loss = criterion(outputs_flat, targets_flat.long())\n",
    "        \n",
    "        #Backward pass/ Compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        #Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #Accumulate loss for the entire batch\n",
    "        total_loss += loss.item() * batch_inputs.size(0)\n",
    "\n",
    "        #Calculate accuracy for the batch\n",
    "        _, predicted = torch.max(outputs, 2)\n",
    "        correct = (predicted == batch_targets).sum().item()\n",
    "        total_correct += correct\n",
    "    \n",
    "    #Calculate average loss and accuracy for the epoch\n",
    "    epoch_loss = total_loss / len(train_loader.dataset)\n",
    "    epoch_accuracy = total_correct / len(train_loader.dataset)\n",
    "\n",
    "    #Stor loss and accuracy values to plot later\n",
    "    loss_values.append(epoch_loss)\n",
    "    accuracy_values.append(epoch_accuracy)\n",
    "    \n",
    "    #Print per-epoch loss and accuracy\n",
    "    print(f'Epoch [{epoch+1}/{max_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot training loss over epochs\n",
    "plt.plot(range(1, max_epochs + 1), loss_values, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Plot training accuracy over epochs\n",
    "plt.plot(range(1, max_epochs + 1), accuracy_values, label='Training Accuracy', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not Original\n",
    "\n",
    "#'seed' is a starting one-hot encoded sequence used as the beginning of each level\n",
    "seed = np.loadtxt('data_preprocessed/seed.txt', dtype=float)[:3*17 - 1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not Original\n",
    "\n",
    "#Converts the one-hot output of the model into a string\n",
    "def onehot_to_string(onehot):\n",
    "    ints = np.argmax(onehot, axis=-1)\n",
    "    chars = [ix_to_char[ix] for ix in ints]\n",
    "    string = \"\".join(chars)\n",
    "    char_array = []\n",
    "    for line in string.rstrip().split('\\n')[:-1]:\n",
    "        if len(line) == 16:\n",
    "            char_array.append(list(line))\n",
    "        elif len(line) > 16:\n",
    "            char_array.append(list(line[:16]))\n",
    "        elif len(line) < 16:\n",
    "            char_array.append(['-'] * (16 - len(line)) + list(line))\n",
    "    char_array = np.array(char_array).T\n",
    "    string = \"\"\n",
    "    for row in char_array:\n",
    "        string += \"\".join(row) + \"\\n\"\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not Original\n",
    "\n",
    "seed[17+14] = 0\n",
    "seed[17+14][char_to_ix['x']] = 1\n",
    "seed[17*2+14] = 0\n",
    "seed[17*2+14][char_to_ix['x']] = 1\n",
    "#The seed in its one-hot form\n",
    "print(seed)\n",
    "#The seed in its character form\n",
    "print(onehot_to_string(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not Original\n",
    "\n",
    "#Returns a copy of the seed in its one-hot encoded form\n",
    "def get_seed():\n",
    "    seed = np.loadtxt('data_preprocessed/seed.txt', dtype=float)[:3*17 - 1]\n",
    "    seed[17+14] = 0\n",
    "    seed[17+14][char_to_ix['x']] = 1\n",
    "    seed[17*2+14] = 0\n",
    "    seed[17*2+14][char_to_ix['x']] = 1\n",
    "    return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not Original\n",
    "num_levels_to_gen = 10\n",
    "num_chunks = 10\n",
    "num_cols_per_chunk = 16\n",
    "num_rows_per_col = 17\n",
    "#Calcuates the number of characters needed to generate a num\n",
    "num_chars_to_gen = num_chunks * num_cols_per_chunk * num_rows_per_col - len(seed)\n",
    "print(num_chars_to_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize empty array to hold generated levels\n",
    "gen = []\n",
    "\n",
    "for i in range(num_levels_to_gen):\n",
    "    \n",
    "    #Seed initialization\n",
    "    seed = torch.from_numpy(get_seed()).type(torch.Tensor).to(device)\n",
    "    \n",
    "    #Initialize empty array to hold generated level sequence\n",
    "    generated_seq = []\n",
    "    \n",
    "    #Initialize hidden states to be empty\n",
    "    h0 = torch.zeros(num_layers, 1, hidden_size).to(device)\n",
    "    c0 = torch.zeros(num_layers, 1, hidden_size).to(device)\n",
    "    \n",
    "    #Generate each character in the level\n",
    "    for j in range(num_chars_to_gen):\n",
    "        #Forward pass through the model\n",
    "        output, (h0, c0) = model(seed.view(1, -1, vocab_size), (h0, c0))\n",
    "        #Get the softmax probabilties of each tile\n",
    "        output_probs = F.softmax(output, dim=-1).squeeze().cpu().detach().numpy()\n",
    "    \n",
    "        #Sample the next character, convert it to an actual char and add it to the end of the generated sequence\n",
    "        next_char_idx = np.random.choice(vocab_size, p=output_probs[-1])\n",
    "        next_char = ix_to_char[next_char_idx]\n",
    "        generated_seq.append(next_char)\n",
    "        \n",
    "        #Add the new char to the seed for the next iteration\n",
    "        next_char_tensor = torch.tensor(next_char_idx).to(device)\n",
    "        one_hot_tensor = F.one_hot(next_char_tensor, num_classes=vocab_size).unsqueeze(0).to(device)\n",
    "        seed = torch.cat((seed[1:].to(device), one_hot_tensor), dim=0)\n",
    "        \n",
    "    #Convert generated sequence to a string, then add the new level to the generated level array\n",
    "    gen.append(''.join(generated_seq))\n",
    "    print(f'Level {i+1}/{num_levels_to_gen} Generated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample of the first level's generated sequence\n",
    "print(gen[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The generated sequence is full of unhelpful \\n and is rotated unhelpfuly, this formats it correctly\n",
    "modified_gen = []\n",
    "for i in range(len(gen)):\n",
    "    rows = gen[i].split('\\n')[1:-1]\n",
    "    for r in range(len(rows)):\n",
    "        row = rows[r]\n",
    "        #If snaking, flip the even rows\n",
    "        if snaking:\n",
    "            if r % 2 == 0:\n",
    "                row = row[::-1]\n",
    "        #Pad the row with 'x' until it reaches length 16\n",
    "        rows[r] = row.ljust(16, 'x')\n",
    "    new_string = ''\n",
    "    for j in range(16):\n",
    "        new_string = new_string + ''.join([row[j] for row in rows])\n",
    "        if j < 15:\n",
    "            new_string += '\\n'\n",
    "    modified_gen.append(new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample of the corrected first level's generated sequence\n",
    "print(modified_gen[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not Original\n",
    "\n",
    "#Saves all of the generated levels to a file\n",
    "for i, g in enumerate(modified_gen):\n",
    "    with open(f'generated_levels_txt/{i+1}.txt', 'w+') as txt_f:\n",
    "        txt_f.write(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
